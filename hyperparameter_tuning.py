import os
import sagemaker
import argparse
from sagemaker.sklearn import SKLearn
from sagemaker.tuner import HyperparameterTuner, IntegerParameter
from smexperiments.experiment import Experiment
from smexperiments.trial import Trial
from smexperiments.search_expression import Filter, Operator, SearchExpression
from smexperiments.trial_component import TrialComponent
from orchestrate import create_and_upload_training_code_package
from sagemaker.analytics import HyperparameterTuningJobAnalytics
from botocore.exceptions import ClientError, ParamValidationError
import argparse
import time
from dotenv import load_dotenv
load_dotenv("./local_credentials.env")


session = sagemaker.Session()
role = os.environ['SAGEMAKER_EXECUTION_ROLE']


def attach_tuner_outputs_to_trial(tuner, trial):
    """Attach trial components generated by a tuner to trial"""
    
    try:
        tuner_job  = tuner.describe()
    except ParamValidationError:
        raise Exception("The tuner job has not finished (or started yet), please try after the job has run")
        
    # the training job names contain the tuning job name (and the training job name is in the source arn)
    source_arn_filter = Filter(
        name="TrialComponentName", operator=Operator.CONTAINS, value=tuner_job["HyperParameterTuningJobName"]
    )

    search_expression = SearchExpression(
        filters=[source_arn_filter]
    )

    # search iterates over every page of results by default
    trial_component_search_results = list(
        TrialComponent.search(search_expression=search_expression)
    )
    
    print(f"Found {len(trial_component_search_results)} trial components.")
    
    # associate the trial components with the trial
    for tc in trial_component_search_results:
        print(f"Associating trial component {tc.trial_component_name} with trial {trial.trial_name}.")
        trial.add_trial_component(tc.trial_component_name)
        # sleep to avoid throttling
        time.sleep(0.5)
        
    return None



if __name__ == '__main__':
    
    parser = argparse.ArgumentParser()

    parser.add_argument("--experiment-name", 
                        type=str, 
                        help="Experiment name for the tuning job e.g. hastie-classification", 
                        default="hastie-classification")
    
    parser.add_argument("--trial-name", type=str, help='Unique name for the trial in the experiment e.g. knn20210831-v0')
    
    args = parser.parse_args()
    
    experiment_name = args.experiment_name
    trial_name = args.trial_name
    
    # Create experiment and trial for the tuning job (Do it before running the job, to save time in case something goes wrong here)
    try: # Create an experiment with given name
        experiment = Experiment.create(experiment_name=experiment_name, 
                                       description='Experiments to classify hastie blobs')
    except ClientError: # Experiment already exists then load the experiment and create a new trial withing that
        experiment = Experiment.load(experiment_name=experiment_name)

    trial = Trial.create(experiment.experiment_name, 
                         trial_name=trial_name)
    
    
    # Create training job Estimator
    bucket = 'hastie'
    file_list = ["train.py", "logger.py"]
    source_code_location = create_and_upload_training_code_package(
            file_list, source_code_package="sourcedir.tar.gz", bucket=bucket
        )

    output_path = "s3://hastie/model/artifacts"
    training_data_location = "s3://hastie/preprocess/data"


    estimator = SKLearn(
            base_job_name='hastie-classification',
            role=role,
            entry_point="train.py",
            framework_version="0.23-1",
            instance_count=1,
            instance_type="ml.m5.large",
            source_dir=source_code_location,
            output_path=output_path,
            use_spot_instances=True,
            max_wait=1800,
            max_run=300,
        )


    # Define custom metric: https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html

    # This must also be emitted by estimator (in train.py): print(f"f1_score = {f1};")
    objective_metric_name = 'f1_score'
    metric_definitions = [{'Name': 'f1_score', 'Regex': "f1_score = (.*?);"}] # Regex must match the format emitted by estimator in train.py

    # Hyperparameters to tune
    hyperparams = {'n_neighbors':IntegerParameter(5,50)}
    
    # Setup tuner
    tuner = HyperparameterTuner(estimator, 
                                objective_metric_name,
                                hyperparams,
                                metric_definitions=metric_definitions,
                                objective_type='Maximize',
                                base_tuning_job_name='hastie-knn-tuning',
                                max_jobs=2,
                                max_parallel_jobs=2,
                                early_stopping_type='Auto')

    tuner.fit(inputs={'training':training_data_location}) # Same channel name as expected in train.py

    
    # Attach tuning job to an experiment    
    attach_tuner_outputs_to_trial(tuner, trial)
    

    # Analysing the runs (alternately look in Sagemaker studio experiments and trials)
#     exp = HyperparameterTuningJobAnalytics(hyperparameter_tuning_job_name=tuner.latest_tuning_job.name)
#     od_jobs = exp.dataframe()
#     od_jobs.sort_values('FinalObjectiveValue', ascending=0)
